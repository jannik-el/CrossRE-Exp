{"related-to": {"precision": 0.125, "recall": 0.027777777777777776, "f1-score": 0.04545454545454545, "support": 72}, "artifact": {"precision": 0.8137535816618912, "recall": 0.9466666666666667, "f1-score": 0.8751926040061633, "support": 300}, "cause-effect": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 1}, "compare": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 8}, "general-affiliation": {"precision": 0.82, "recall": 0.875, "f1-score": 0.8466076696165191, "support": 328}, "named": {"precision": 0.3795620437956204, "recall": 0.5531914893617021, "f1-score": 0.45021645021645024, "support": 94}, "opposite": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 14}, "origin": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 47}, "part-of": {"precision": 0.375, "recall": 0.15625, "f1-score": 0.22058823529411764, "support": 96}, "physical": {"precision": 0.6775956284153005, "recall": 0.7126436781609196, "f1-score": 0.6946778711484592, "support": 174}, "role": {"precision": 0.5833333333333334, "recall": 0.6640316205533597, "f1-score": 0.6210720887245841, "support": 253}, "social": {"precision": 0.5, "recall": 0.5319148936170213, "f1-score": 0.5154639175257731, "support": 47}, "temporal": {"precision": 0.5128205128205128, "recall": 0.6896551724137931, "f1-score": 0.5882352941176471, "support": 29}, "topic": {"precision": 0.125, "recall": 0.047619047619047616, "f1-score": 0.06896551724137931, "support": 42}, "type-of": {"precision": 0.6875, "recall": 0.6111111111111112, "f1-score": 0.6470588235294118, "support": 18}, "usage": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 2}, "win-defeat": {"precision": 0.7619047619047619, "recall": 0.8247422680412371, "f1-score": 0.792079207920792, "support": 97}, "micro avg": {"precision": 0.6729559748427673, "recall": 0.6596794081381011, "f1-score": 0.6662515566625156, "support": 1622}, "macro avg": {"precision": 0.37420410952537764, "recall": 0.3906237485483903, "f1-score": 0.37444777792916717, "support": 1622}, "weighted avg": {"precision": 0.6098344242105475, "recall": 0.6596794081381011, "f1-score": 0.627423397295706, "support": 1622}, "samples avg": {"precision": 0.6729559748427673, "recall": 0.6653039832285116, "f1-score": 0.6678197064989517, "support": 1622}}