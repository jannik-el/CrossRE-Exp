{"related-to": {"precision": 0.28273809523809523, "recall": 0.22946859903381642, "f1-score": 0.2533333333333333, "support": 414}, "artifact": {"precision": 0.6647791619479049, "recall": 0.9244094488188976, "f1-score": 0.7733860342555995, "support": 635}, "cause-effect": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 86}, "compare": {"precision": 0.6666666666666666, "recall": 0.07317073170731707, "f1-score": 0.13186813186813187, "support": 82}, "general-affiliation": {"precision": 0.7228668941979522, "recall": 0.8547215496368039, "f1-score": 0.7832840236686393, "support": 1239}, "named": {"precision": 0.7560483870967742, "recall": 0.6720430107526881, "f1-score": 0.7115749525616697, "support": 558}, "opposite": {"precision": 0.29577464788732394, "recall": 0.1640625, "f1-score": 0.21105527638190955, "support": 128}, "origin": {"precision": 0.47333333333333333, "recall": 0.23355263157894737, "f1-score": 0.31277533039647576, "support": 304}, "part-of": {"precision": 0.2976054732041049, "recall": 0.361997226074896, "f1-score": 0.3266583229036295, "support": 721}, "physical": {"precision": 0.6248288452761296, "recall": 0.940893470790378, "f1-score": 0.7509599561162917, "support": 1455}, "role": {"precision": 0.6406995230524642, "recall": 0.4017946161515454, "f1-score": 0.49387254901960786, "support": 2006}, "social": {"precision": 0.5, "recall": 0.2761904761904762, "f1-score": 0.3558282208588957, "support": 105}, "temporal": {"precision": 0.7427293064876958, "recall": 0.8137254901960784, "f1-score": 0.7766081871345031, "support": 408}, "topic": {"precision": 0.2, "recall": 0.011764705882352941, "f1-score": 0.022222222222222223, "support": 170}, "type-of": {"precision": 0.9186046511627907, "recall": 0.23511904761904762, "f1-score": 0.37440758293838866, "support": 336}, "usage": {"precision": 0.8, "recall": 0.14, "f1-score": 0.23829787234042554, "support": 200}, "win-defeat": {"precision": 0.6587837837837838, "recall": 0.6151419558359621, "f1-score": 0.6362153344208809, "support": 317}, "micro avg": {"precision": 0.61317489616982, "recall": 0.5799869052815364, "f1-score": 0.5961193360251235, "support": 9164}, "macro avg": {"precision": 0.5438505158432365, "recall": 0.40870914472171804, "f1-score": 0.4207263135541532, "support": 9164}, "weighted avg": {"precision": 0.6077124088502444, "recall": 0.5799869052815364, "f1-score": 0.5618127682452486, "support": 9164}, "samples avg": {"precision": 0.61317489616982, "recall": 0.588332564220889, "f1-score": 0.5965812951853561, "support": 9164}}