{"related-to": {"precision": 0.27571115973741794, "recall": 0.30434782608695654, "f1-score": 0.2893226176808266, "support": 414}, "artifact": {"precision": 0.7715877437325905, "recall": 0.8724409448818897, "f1-score": 0.8189209164818921, "support": 635}, "cause-effect": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 86}, "compare": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 82}, "general-affiliation": {"precision": 0.728454172366621, "recall": 0.8595641646489104, "f1-score": 0.7885968159940762, "support": 1239}, "named": {"precision": 0.6981981981981982, "recall": 0.5555555555555556, "f1-score": 0.6187624750499002, "support": 558}, "opposite": {"precision": 0.125, "recall": 0.0234375, "f1-score": 0.039473684210526314, "support": 128}, "origin": {"precision": 0.4819277108433735, "recall": 0.2631578947368421, "f1-score": 0.3404255319148936, "support": 304}, "part-of": {"precision": 0.27125506072874495, "recall": 0.4646324549237171, "f1-score": 0.34253578732106343, "support": 721}, "physical": {"precision": 0.6556829035339063, "recall": 0.9436426116838488, "f1-score": 0.7737390814313891, "support": 1455}, "role": {"precision": 0.6257166257166257, "recall": 0.38085742771684944, "f1-score": 0.4735048032228075, "support": 2006}, "social": {"precision": 0.5217391304347826, "recall": 0.11428571428571428, "f1-score": 0.1875, "support": 105}, "temporal": {"precision": 0.771889400921659, "recall": 0.821078431372549, "f1-score": 0.7957244655581948, "support": 408}, "topic": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 170}, "type-of": {"precision": 0.9545454545454546, "recall": 0.25, "f1-score": 0.39622641509433965, "support": 336}, "usage": {"precision": 1.0, "recall": 0.03, "f1-score": 0.058252427184466014, "support": 200}, "win-defeat": {"precision": 0.7668918918918919, "recall": 0.7160883280757098, "f1-score": 0.7406199021207178, "support": 317}, "micro avg": {"precision": 0.608444854637748, "recall": 0.5755128764731559, "f1-score": 0.591520861372813, "support": 9164}, "macro avg": {"precision": 0.508741144273604, "recall": 0.3881816972922672, "f1-score": 0.39197676019206434, "support": 9164}, "weighted avg": {"precision": 0.6107687185724805, "recall": 0.5755128764731559, "f1-score": 0.5584012121667564, "support": 9164}, "samples avg": {"precision": 0.608444854637748, "recall": 0.5831410552222734, "f1-score": 0.5915436086755884, "support": 9164}}